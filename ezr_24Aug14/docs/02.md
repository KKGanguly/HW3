# Active Learning

- Assumes abundant access to examples, 
  - but very limited access to oracles that can label each example 
- A repeated result:
  - Learners learn better (using fewer labels)
  - if they can decode for them selves what examples to use.
- Goal:
  - build a competent model using as few labels as followed.

## Tricks: Often cheaper, faster, to find "X" than "Y"

- $Y=f(X)$
  - $X$,$Y$ are our independent and dependent variables.
  - $f$ is the thing we are trying to find
- e.g. Fishing: 
  - Glance up and down the river. 
  - That looks like a good spot. 
  - 3 hours later:  well, it was not
- e.g. Used car yard: 
   -  Glancing over 100 cars:  count the cars and their colors and number of wheels and size of car.
   -  But to find gas mileage-- got to take each out for a long drive.

## SE Examples where finding $X$ is  cheaper than $Y$

- $X$,$Y$ are our independent and dependent variables.
- Quick to mine $X$ GitHub to get code size, dependencies per function,  
  - Slow to get $Y$ (a) development time, (b) what people will pay for it
- Quick to count $X$ the number of classes in a system. 
  - Slow to get  $Y$ an organization to tell you human effort to build and maintain that code.
- Quick to enumerate $X$ many  design options (20 yes-no = $2^{20}$ options) 
  - Slow to check $Y$ those options with   the human stakeholders.
- Quick to list $X$ configuration parameters for  the  software. 
  - Slow to find $X$ runtime and energy requirements for all configurations.
- Quick to list $X$ data miner params (e.g. how many neighbors in knn?) 
  - Slow to find  $Y$ best setting for local data. 
- Quick to  make $X$ test case inputs using (e.g.) random input selection
  - Slow to run all tests and  get $Y$ humans to check each output 

## This is called "Active Learning"

- Learning works better if the learner can pick its training data[^brochu].
Given two models that predict for good $g$ or bad $b$:
- An active learning loop:

## An Active Learning Loop

- _Labelling_: given an example with $X$, but not $Y$, get the $Y$.
- Just for simplicity, assume we a model can inputs $X$ values to predict for good $g$ or bad $b$:

|n|Task | Notes|
|-:|-----|------|
|1|Sample a little  | Get a get a few $Y$ values (picked at random?) |
|2|Learn a little   | Build a tiny model from that sample|
|3| Reflect | Compute $b,r$|
|4| Acquire         | Label an example that (e.g.) maximizes $b/r$. Add it to the sample|
|5| Repeat          | Goto 2|

How to 
- Sample, once
  - Use reflection to find one unlabelled thingFind &$ the $X$ variables, 
  - guess what might be the next most informative example
  - get its $Y$ value, .

## Three kinds of active learning
- pool-based: expoores a large pool of unlabelled examples

A  common distinction in the active learning literature~\cite{Hospedales22} is between  pool-based active learning (where the supply of unlabeled examples is very large) and stream-based active learning (where the pool
divides into small buffers, and each learning session on has access to the next buffer).
\item Another distinction is for  {\em model query synthesis}, or MQS~\cite{BUDD2021102062}
where the example to be labeled is inferred using feature weightings taken from inside the classifier.   Active learners can use MQS to simplify what they show their oracles by focusing on just a few features (those with most weight). 
\item There is much research on appropriate acquisition functions (see ~\cite{Bilal20} for a succinct review
of that work). Some acquisition functions (such as UCB~\cite{brochu2010tutorial}) reflect not only on the prediction
for each unlabeled example, bur also the variance in that prediction. Other acquisition
functions are adaptive; i.e. as more labels are gathered they change from exploring regions of high variance to exploiting regions of high mean predicted value. 
\end{itemize}


- Do it simpler, faster. using fewer resources?
- Know how to combine things, such that you can more with less?
- Teach seemingly  complex things to newbies?

## Tricks: Often cheaper, faster, to find "X" than "Y"

- $Y=f(X)$
  - $X$,$Y$ are our independent and dependent variables.
  - $f$ is the thing we are trying to find
- e.g. Fishing: 
  - Glance up and down the river. 
  - That looks like a good spot. 
  - 3 hours later:  well, it was not
- e.g. Used car yard: 
   -  Glancing over 100 cars:  count the cars and their colors and number of wheels and size of car.
   -  But to find gas mileage-- got to take each out for a long drive.

## SE Examples where finding $X$ is  cheaper than $Y$

- $X$,$Y$ are our independent and dependent variables.
- Quick to mine $X$ GitHub to get code size, dependencies per function,  
  - Slow to get $Y$ (a) development time, (b) what people will pay for it
- Quick to count $X$ the number of classes in a system. 
  - Slow to get  $Y$ an organization to tell you human effort to build and maintain that code.
- Quick to enumerate $X$ many  design options (20 yes-no = $2^{20}$ options) 
  - Slow to check $Y$ those options with   the human stakeholders.
- Quick to list $X$ configuration parameters for  the  software. 
  - Slow to find $X$ runtime and energy requirements for all configurations.
- Quick to list $X$ data miner params (e.g. how many neighbors in knn?) 
  - Slow to find  $Y$ best setting for local data. 
- Quick to  make $X$ test case inputs using (e.g.) random input selection
  - Slow to run all tests and  get $Y$ humans to check each output 

## This is called "Active Learning"

- Learning works better if the learner can pick its training data[^brochu].
Given two models that predict for good $g$ or bad $b$:
- An active learning loop:

## An Active Learning Loop

- _Labelling_: given an example with $X$, but not $Y$, get the $Y$.
- Just for simplicity, assume we a model can inputs $X$ values to predict for good $g$ or bad $b$:

|n|Task | Notes|
|-:|-----|------|
|1|Sample a little  | Get a get a few $Y$ values (picked at random?) |
|2|Learn a little   | Build a tiny model from that sample|
|3| Reflect | Compute $b,r$|
|4| Acquire         | Label an example that (e.g.) maximizes $b/r$. Add it to the sample|
|5| Repeat          | Goto 2|

How to 
- Sample, once
  - Use reflection to find one unlabelled thingFind &$ the $X$ variables, 
  - guess what might be the next most informative example
  - get its $Y$ value, .

## Three kinds of active learning

